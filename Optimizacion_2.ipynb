{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import History \n",
    "#using tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cases_per_day(fecha_sintomas,graph=False,ret=False):\n",
    "    \"\"\"\n",
    "    args: \n",
    "    -fecha_sintomas: day of beginning of symptomps for each case, can't be NaN or NaT (already processed)\n",
    "    -graph: defaults to False. if True, makes graph of new cases per day \n",
    "    -ret: defaults to False. if True, returns days-cases per day  np.array casos_por_dia\n",
    "    given the days of beginning of symptoms, processes to get np.array of cases per day and days since epidemic\n",
    "    can graph and return this vector with the defaults explained before\n",
    "    \"\"\"\n",
    "    counts = np.bincount(fecha_sintomas)\n",
    "    ultima_fecha = max(fecha_sintomas)\n",
    "    aux = range(ultima_fecha+1)\n",
    "    casos_por_dia = np.vstack((aux,counts[aux])).T #(days since inicio_epidemia x (fecha_inicio_sintomas == days))\n",
    "    if graph == True:\n",
    "        plt.xlabel(\"Dia\")\n",
    "        plt.ylabel(\"Casos\")\n",
    "        plt.axvspan(ultima_fecha-10, ultima_fecha+3, facecolor='r', alpha=0.5,label=\"últimos 10 días\")\n",
    "        plt.plot(casos_por_dia[:,0],casos_por_dia[:,1],'-ob',label=\"casos hasta dia: \"+str(ultima_fecha))\n",
    "        plt.style.use('ggplot')\n",
    "        plt.legend()\n",
    "        #plt.savefig(\"casos_por_dia_bariloche.pdf\")\n",
    "    if ret == True:\n",
    "        return casos_por_dia   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_cases(fecha_sintomas,graph=False,ret=False):\n",
    "    \"\"\"\n",
    "    args: \n",
    "    -fecha_sintomas: day of beginning of symptomps for each case, can't be NaN or NaT (already processed)\n",
    "    -graph: defaults to False. if True, makes graph of new cases per day \n",
    "    -ret: defaults to False. if True, returns days-cumulative cases per day np array\n",
    "    given the days of beginning of symptoms, processes to get np.array of cumulative cases per day and days since epidemic\n",
    "    can graph and return this vector with the defaults explained before\n",
    "    \"\"\"\n",
    "    ultima_fecha = max(fecha_sintomas)\n",
    "    casos_por_dia = new_cases_per_day(fecha_sintomas,ret=True)\n",
    "    casos_acumulados_por_dia = np.copy(casos_por_dia)\n",
    "    casos_acumulados_por_dia[:,1] = np.cumsum(casos_por_dia[:,1]) \n",
    "    if graph == True:\n",
    "        plt.xlabel(\"Dia\")\n",
    "        plt.ylabel(\"Casos acumulados\")\n",
    "        plt.axvspan(ultima_fecha-10, ultima_fecha+3, facecolor='r', alpha=0.5,label=\"últimos 10 días\")\n",
    "        plt.scatter(casos_acumulados_por_dia[:,0],casos_acumulados_por_dia[:,1],c='b',label=\"casos acumulados hasta dia: \"+str(ultima_fecha))\n",
    "        plt.legend()\n",
    "        plt.style.use('ggplot')\n",
    "        plt.savefig(\"casos_acumulados_bariloche.pdf\")\n",
    "    if ret == True:\n",
    "        return casos_acumulados_por_dia   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repratio_t_conv(fecha_sintomas,a,graph=False,ret=False):\n",
    "    \"\"\"\n",
    "    args: \n",
    "    -fecha_sintomas: day of beginning of symptomps for each case, can't be NaN or NaT (already processed)\n",
    "    -graph: defaults to False. if True, makes graph of rep ratio per day\n",
    "    -ret: defaults to False. if True, returns reproductive ratio per day np array of (days,r_days) and\n",
    "    the n_t vector which is used to calculate it, which is extension of new cases per day made as \n",
    "    necessary, by taking means and not linear regression. \n",
    "    given the days of beginning of symptoms, processes to get np.array of cumulative cases per day and days since epidemic\n",
    "    can graph and return this vector with the defaults explained before for days (0,lastday+4)\n",
    "    conventional version, which means no coef a,b,c,d,e. r_t valid for days >= 6. \n",
    "    \"\"\"\n",
    "    casos_por_dia = new_cases_per_day(fecha_sintomas,ret=True)\n",
    "    n_t = np.copy(casos_por_dia) #here we store data + proyection for days t+4 \n",
    "    prox_dia = max(fecha_sintomas) + 1\n",
    "    #completing the values for the rest of the values of n_t\n",
    "    aux = np.zeros(2)\n",
    "    i = 0\n",
    "    while i < 4: #completing until the day t+4 bc im gonna need it later for averaging in 7 days\n",
    "        aux[0] = prox_dia + i\n",
    "        aux[1] = np.mean(n_t[prox_dia-7:,1])\n",
    "        n_t  = np.vstack((n_t,aux)) #extending n_t up to day t+1\n",
    "        i= i +1\n",
    "    #create storage and calculate values for r_t\n",
    "    r_t = np.copy(n_t) #here we will store the rt\n",
    "    r_t = r_t.astype(float)\n",
    "    dias_aux = np.arange(0,prox_dia+3)\n",
    "    for i in dias_aux[dias_aux>=6]:\n",
    "        aux = a[0]*n_t[i-6,1] + a[1]*n_t[i-5,1] + a[2]*n_t[i-4,1] #denominator of the r_t expression\n",
    "        if(aux==0):\n",
    "            aux = 1 #the first cases in which the denominator is == 0\n",
    "        r_t[i,1] = min(((a[3]*n_t[i-1,1] + a[4]*n_t[i,1] + a[5]*n_t[i+1,1])/(aux)),4) #the rest of the days with limit value\n",
    "    if graph == True:\n",
    "        plt.style.use('ggplot')\n",
    "        plt.axvspan(prox_dia-10, prox_dia+3, facecolor='r', alpha=0.5,label=\"últimos 10 días\")\n",
    "        plt.scatter(r_t[6:prox_dia,0],r_t[6:prox_dia,1],c='b',label=r'$r_t$')\n",
    "        plt.xlabel(\"Dia\")\n",
    "        plt.ylabel(r'$r_t$',fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"rt_raw_casos.pdf\")\n",
    "    if ret == True:\n",
    "        return n_t,r_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_diagram(fecha_sintomas,pop,a,rep_fun=repratio_t_conv,graph=False,ret=False):\n",
    "    \"\"\"\n",
    "    args: \n",
    "    -fecha_sintomas: day of beginning of symptomps for each case, can't be NaN or NaT (already processed)\n",
    "    -pop: number of inhabitants of the region/population of interest\n",
    "    -rep_fun: function to calculate empirical reproductive ratio. defaults tp repratio_t_conv (not optimized)\n",
    "    -graph: defaults to False. if True, makes graph of risk diagram\n",
    "    -ret: defaults to False. if True, returns the components of risk diagram in two np.arrays, which are\n",
    "     the attack ratio (days>=20) as well as the rep ratio averaged over 7 days (days>=20). \n",
    "    \"\"\"\n",
    "    #we proceed to calculate what makes a risk diagram then\n",
    "    #first, we will calculate r_t average in 7 days\n",
    "    prox_dia = max(fecha_sintomas) + 1\n",
    "    n_t,r_t = rep_fun(fecha_sintomas,a,False,True)\n",
    "    r_t_seven = np.zeros(prox_dia) #real values for index>=9\n",
    "    a_t = np.zeros(prox_dia) #real values for index >= 13\n",
    "    i = 9\n",
    "    while i < prox_dia:\n",
    "        r_t_seven[i] = np.mean(r_t[i-3:i+4,1])\n",
    "        i = i+1\n",
    "    i = 13\n",
    "    while i < prox_dia:\n",
    "        a_t[i] = np.sum(n_t[i-13:i+1,1]) \n",
    "        i = i+1\n",
    "    a_t = a_t * (100000/pop)\n",
    "    if graph == True:\n",
    "        plt.plot(a_t[13:],r_t_seven[13:],'-or',markersize=5) #not at all fancy risk diagram,looks reasonable\n",
    "        plt.xlabel(r'$A_{t}^{14}$')\n",
    "        plt.ylabel(r'$R_{t}^{7}$')\n",
    "        plt.style.use('ggplot')\n",
    "        plt.title('Diagrama de riesgo Bariloche',fontsize=10)\n",
    "        plt.savefig('riskdiagram_brc.pdf')\n",
    "    if ret == True:\n",
    "        return r_t_seven,a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_last14(casos_por_dia):\n",
    "    ac_por_dia = np.copy(casos_por_dia)\n",
    "    for i in casos_por_dia[:,0]:\n",
    "        index = np.copy(casos_por_dia[casos_por_dia[:,0]<i+1])\n",
    "        index = index[i-13<=index[:,0]]\n",
    "        index = index[:,1]\n",
    "        ac_por_dia[i,1] = np.sum(index) \n",
    "    return ac_por_dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cases(fecha_sintomas,pop,a,rep_fun=repratio_t_conv,graph=False,ret=False):\n",
    "    \"\"\"\n",
    "    args: \n",
    "    -fecha_sintomas: day of beginning of symptomps for each case, can't be NaN or NaT (already processed)\n",
    "    -pop: number of inhabitants of the region/population of interest\n",
    "    -rep_fun: function to calculate empirical reproductive ratio. defaults tp repratio_t_conv (not optimized)\n",
    "    -graph: defaults to False. if True, makes graph of risk diagram\n",
    "    -ret: defaults to False. if True, returns the predictions and cases per day\n",
    "    predicts cases per day after calculating the risk diagram, using the empirical reproductive\n",
    "    ratio given by rep_fun\n",
    "    \"\"\"\n",
    "    prox_dia = max(fecha_sintomas)+1\n",
    "    casos_por_dia = new_cases_per_day(fecha_sintomas,ret=True)\n",
    "    r_def,a_t = risk_diagram(fecha_sintomas,pop,a,rep_fun,ret=True)\n",
    "    r_def = r_def[13:]\n",
    "    a_t = a_t[13:]\n",
    "    p_t = r_def * a_t\n",
    "    valid_days = np.arange(19,prox_dia+6,1)\n",
    "    casos_por_dia = sum_last14(casos_por_dia) #gets all active cases in the last 14-days \n",
    "    if graph == True:\n",
    "        plt.style.use('ggplot')\n",
    "        plt.xlabel('Dias desde el comienzo')\n",
    "        plt.ylabel('Infectados activos en BRC')\n",
    "        plt.plot(casos_por_dia[:,0],casos_por_dia[:,1]*(100000/pop),'-ob',markersize=4,label=\"casos hasta dia: \"+str(prox_dia))\n",
    "        plt.plot(valid_days,p_t,linewidth=3,label='prediccion')\n",
    "        plt.legend()\n",
    "        #plt.savefig('9oct_predic_casos.pdf')\n",
    "    if ret == True:\n",
    "        error_global = np.linalg.norm(casos_por_dia[19:,1]-p_t[:-6])\n",
    "        return error_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dates(df):\n",
    "    #changes fecha_inicio_sintomas according to new criteria\n",
    "    filt_df1 = (df.fecha_inicio_sintomas.isnull()) #filter fecha_inicio_sintomas = inexistant\n",
    "    df_sin_fecha = df.loc[filt_df1]\n",
    "    n_size = df_sin_fecha.shape[0]\n",
    "    df.loc[filt_df1,\"fecha_inicio_sintomas\"] = df.loc[filt_df1,\"fecha_apertura\"] - np.random.randint(0,9,n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"casos_Bariloche.txt\",sep=\",\",quotechar='\"',\n",
    "                   parse_dates=[\"fecha_inicio_sintomas\",\"fecha_apertura\"],na_values=['']) #data loading\n",
    "df = pd.DataFrame(data) #converting to dataframe for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primer sintoma de persona confirmada:  2020-03-09 00:00:00\n",
      "ultimo sintoma de persona confirmada:  2020-10-24 00:00:00\n",
      "ultima apertura de persona confirmada:  2020-10-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "inicio_epidemia = min(df[\"fecha_inicio_sintomas\"]) #first symptoms of a person registered\n",
    "ultima_actualizacion_sintomas = max(df[\"fecha_inicio_sintomas\"]) #last day symptoms of a person registered\n",
    "ultima_actualizacion_apertura = max(df[\"fecha_apertura\"])\n",
    "df[\"fecha_inicio_sintomas\"] -= inicio_epidemia #correcting by inicio_epidemia \n",
    "df[\"fecha_apertura\"] -= inicio_epidemia #correcting by inicio_epidemia\n",
    "df.fecha_inicio_sintomas = df.fecha_inicio_sintomas.dt.days #change to int, ditch days \n",
    "df.fecha_apertura = df.fecha_apertura.dt.days #change to int, ditch days\n",
    "change_dates(df) #replaces non existing fecha_inicio_sintomas acc to new criteria\n",
    "print(\"primer sintoma de persona confirmada: \",inicio_epidemia)\n",
    "print(\"ultimo sintoma de persona confirmada: \",ultima_actualizacion_sintomas)\n",
    "print(\"ultima apertura de persona confirmada: \",ultima_actualizacion_apertura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_sintomas = df.fecha_inicio_sintomas.to_numpy() #numpy array of fecha_inicio_sintomas\n",
    "fecha_apertura = df.fecha_apertura.to_numpy() #numpy array of fecha_apertura\n",
    "fecha_sintomas = fecha_sintomas.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(6) + 1\n",
    "pop = 100000\n",
    "rep_fun = repratio_t_conv\n",
    "r_def,a_t = risk_diagram(fecha_sintomas,pop,a,rep_fun,ret=True)\n",
    "a_t = a_t[13:]\n",
    "a_t[a_t.shape[0]-7:] = 0\n",
    "a = np.zeros(a_t.shape[0]+10)\n",
    "a[:a_t.shape[0]] = a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_por_dia = new_cases_per_day(fecha_sintomas,ret=True)\n",
    "casos_por_dia = sum_last14(casos_por_dia)\n",
    "casos_por_dia = casos_por_dia[20:,1]\n",
    "b = np.zeros(a_t.shape[0]+10)\n",
    "b[:casos_por_dia.shape[0]] = casos_por_dia\n",
    "b = b.reshape(b.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_por_dia = new_cases_per_day(fecha_sintomas,ret=True)\n",
    "n_t = np.copy(casos_por_dia)\n",
    "prox_dia = max(fecha_sintomas) + 1\n",
    "#completing the values for the rest of the values of n_t\n",
    "aux = np.zeros(2)\n",
    "i = 0\n",
    "while i < 4: #completing until the day t+4 bc im gonna need it later for averaging in 7 days\n",
    "    aux[0] = prox_dia + i\n",
    "    aux[1] = np.mean(n_t[prox_dia-7:,1])\n",
    "    n_t  = np.vstack((n_t,aux)) #extending n_t up to day t+1\n",
    "    i= i +1\n",
    "dias = np.copy(n_t[:,0]).astype(int)\n",
    "dias = dias[:-1]\n",
    "m1 = np.zeros((dias[dias>=6].shape[0],3)) #denominator of r7 expression\n",
    "m2 = np.zeros((dias[dias>=6].shape[0],3)) #numerator of r7 expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dias[dias>=6]:\n",
    "    m1[i-6,:] = n_t[i-6:i-3,1]\n",
    "    m2[i-6,:] = n_t[i-1:i+2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dias = m1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_matrix(shape):\n",
    "    n_dias = shape\n",
    "    m = np.zeros((n_dias,n_dias))\n",
    "    i = 0\n",
    "    while i < n_dias - 7:\n",
    "        m[i,i:i+7] = 1/7\n",
    "        i = i + 1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = m1[11:,:]\n",
    "m2 = m2[11:,:]\n",
    "n_dias = m1.shape[0]\n",
    "a = a[11:]\n",
    "b = b[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyA = my_matrix(n_dias)\n",
    "numpyA = keras.backend.variable(numpyA)\n",
    "mat = numpyA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "216/216 [==============================] - 0s 217us/step - loss: 97250.1328 - mse: 97250.1328\n",
      "Epoch 2/100\n",
      "216/216 [==============================] - 0s 29us/step - loss: 87321.8125 - mse: 87321.8125\n",
      "Epoch 3/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 75273.2656 - mse: 75273.2656\n",
      "Epoch 4/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 64162.5625 - mse: 64162.5586\n",
      "Epoch 5/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 63757.8203 - mse: 63757.8203\n",
      "Epoch 6/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 63994.6680 - mse: 63994.6680\n",
      "Epoch 7/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 73366.3438 - mse: 73366.3438\n",
      "Epoch 8/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 64232.9141 - mse: 64232.9102\n",
      "Epoch 9/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 60636.0234 - mse: 60636.0234\n",
      "Epoch 10/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 64394.1172 - mse: 64394.1172\n",
      "Epoch 11/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 44708.5078 - mse: 44708.5078\n",
      "Epoch 12/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 49581.0547 - mse: 49581.0547\n",
      "Epoch 13/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 55351.0039 - mse: 55351.0039\n",
      "Epoch 14/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 49327.0078 - mse: 49327.0078\n",
      "Epoch 15/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 34692.1797 - mse: 34692.1797\n",
      "Epoch 16/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 70215.1562 - mse: 70215.1562\n",
      "Epoch 17/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 49103.7500 - mse: 49103.7500\n",
      "Epoch 18/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 49332.2578 - mse: 49332.2578\n",
      "Epoch 19/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 38595.3711 - mse: 38595.3711\n",
      "Epoch 20/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 49174.4180 - mse: 49174.4180\n",
      "Epoch 21/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 40518.3633 - mse: 40518.3594\n",
      "Epoch 22/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 42783.2188 - mse: 42783.2188\n",
      "Epoch 23/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 40638.6133 - mse: 40638.6094\n",
      "Epoch 24/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 33537.3789 - mse: 33537.3789\n",
      "Epoch 25/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 32824.4570 - mse: 32824.4570\n",
      "Epoch 26/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 37242.7031 - mse: 37242.7031\n",
      "Epoch 27/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 28001.7812 - mse: 28001.7793\n",
      "Epoch 28/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 34441.8672 - mse: 34441.8672\n",
      "Epoch 29/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 39606.3516 - mse: 39606.3516\n",
      "Epoch 30/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 45796.2891 - mse: 45796.2852\n",
      "Epoch 31/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 42380.8281 - mse: 42380.8281\n",
      "Epoch 32/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 41031.5977 - mse: 41031.5977\n",
      "Epoch 33/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 39865.5664 - mse: 39865.5664\n",
      "Epoch 34/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 38721.6523 - mse: 38721.6523\n",
      "Epoch 35/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 37241.7070 - mse: 37241.7070\n",
      "Epoch 36/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 35115.0586 - mse: 35115.0586\n",
      "Epoch 37/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 26815.7500 - mse: 26815.7500\n",
      "Epoch 38/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 40704.5781 - mse: 40704.5781\n",
      "Epoch 39/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 41256.5742 - mse: 41256.5742\n",
      "Epoch 40/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 27972.4844 - mse: 27972.4844\n",
      "Epoch 41/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 36575.3711 - mse: 36575.3711\n",
      "Epoch 42/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 32877.1562 - mse: 32877.1562\n",
      "Epoch 43/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 29513.1289 - mse: 29513.1289\n",
      "Epoch 44/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 41873.0742 - mse: 41873.0742\n",
      "Epoch 45/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 36956.8789 - mse: 36956.8789\n",
      "Epoch 46/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 36988.7734 - mse: 36988.7734\n",
      "Epoch 47/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 25094.3516 - mse: 25094.3516\n",
      "Epoch 48/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 23895.3359 - mse: 23895.3359\n",
      "Epoch 49/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 25488.9824 - mse: 25488.9824\n",
      "Epoch 50/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 28206.8535 - mse: 28206.8535\n",
      "Epoch 51/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 23939.5254 - mse: 23939.5254\n",
      "Epoch 52/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 26786.8594 - mse: 26786.8594\n",
      "Epoch 53/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 39894.5898 - mse: 39894.5898\n",
      "Epoch 54/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 31531.1445 - mse: 31531.1426\n",
      "Epoch 55/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 27022.1113 - mse: 27022.1113\n",
      "Epoch 56/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 29761.6445 - mse: 29761.6426\n",
      "Epoch 57/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 31904.6113 - mse: 31904.6113\n",
      "Epoch 58/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 22907.6738 - mse: 22907.6738\n",
      "Epoch 59/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 23123.1211 - mse: 23123.1211\n",
      "Epoch 60/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 24824.5879 - mse: 24824.5879\n",
      "Epoch 61/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 31745.6660 - mse: 31745.6660\n",
      "Epoch 62/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 34388.9531 - mse: 34388.9531\n",
      "Epoch 63/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 28705.7734 - mse: 28705.7734\n",
      "Epoch 64/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 23261.3652 - mse: 23261.3652\n",
      "Epoch 65/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 25696.5312 - mse: 25696.5293\n",
      "Epoch 66/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 25195.1914 - mse: 25195.1914\n",
      "Epoch 67/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 22377.7969 - mse: 22377.7969\n",
      "Epoch 68/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 24964.1133 - mse: 24964.1133\n",
      "Epoch 69/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 26824.3340 - mse: 26824.3340\n",
      "Epoch 70/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 25900.0977 - mse: 25900.0977\n",
      "Epoch 71/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 32949.7227 - mse: 32949.7227\n",
      "Epoch 72/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 24559.7461 - mse: 24559.7461\n",
      "Epoch 73/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 24719.9961 - mse: 24719.9961\n",
      "Epoch 74/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 21722.9336 - mse: 21722.9336\n",
      "Epoch 75/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 26214.6230 - mse: 26214.6230\n",
      "Epoch 76/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 22724.9375 - mse: 22724.9375\n",
      "Epoch 77/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 29385.3008 - mse: 29385.3008\n",
      "Epoch 78/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 30335.5605 - mse: 30335.5605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 28950.8516 - mse: 28950.8516\n",
      "Epoch 80/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 31617.4902 - mse: 31617.4902\n",
      "Epoch 81/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 27952.7188 - mse: 27952.7168\n",
      "Epoch 82/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 24683.1367 - mse: 24683.1367\n",
      "Epoch 83/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 26590.5215 - mse: 26590.5215\n",
      "Epoch 84/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 25923.4414 - mse: 25923.4414\n",
      "Epoch 85/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 22186.4570 - mse: 22186.4551\n",
      "Epoch 86/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 20744.6621 - mse: 20744.6621\n",
      "Epoch 87/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 30586.4258 - mse: 30586.4258\n",
      "Epoch 88/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 24994.0039 - mse: 24994.0039\n",
      "Epoch 89/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 21123.4492 - mse: 21123.4492\n",
      "Epoch 90/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 31900.0352 - mse: 31900.0352\n",
      "Epoch 91/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 23426.8652 - mse: 23426.8652\n",
      "Epoch 92/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 30369.6113 - mse: 30369.6113\n",
      "Epoch 93/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 35223.9336 - mse: 35223.9336\n",
      "Epoch 94/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 25676.2148 - mse: 25676.2148\n",
      "Epoch 95/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 21527.0781 - mse: 21527.0781\n",
      "Epoch 96/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 22221.7969 - mse: 22221.7969\n",
      "Epoch 97/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 34059.3086 - mse: 34059.3086\n",
      "Epoch 98/100\n",
      "216/216 [==============================] - 0s 37us/step - loss: 28706.2324 - mse: 28706.2324\n",
      "Epoch 99/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 31953.5664 - mse: 31953.5664\n",
      "Epoch 100/100\n",
      "216/216 [==============================] - 0s 0us/step - loss: 26353.1289 - mse: 26353.1289\n",
      "Model: \"model_85\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_256 (InputLayer)          (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_257 (InputLayer)          (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_169 (Dense)               (None, 1)            3           input_256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 1)            3           input_257[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_169 (Lambda)             (None, 1)            0           dense_169[0][0]                  \n",
      "                                                                 dense_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_258 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_170 (Lambda)             (216, 1)             0           lambda_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_85 (Multiply)          (216, 1)             0           input_258[0][0]                  \n",
      "                                                                 lambda_170[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 3\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = keras.layers.Input(shape=(3,))\n",
    "input_2 = keras.layers.Input(shape=(3,))\n",
    "input_3 = keras.layers.Input(shape=(1,))\n",
    "initializer = keras.initializers.Ones() #the vector of weights a\n",
    "initializer_2 = keras.initializers.Ones()\n",
    "l_input_1 = keras.layers.Dense(1,activation='linear',kernel_initializer=initializer_2,use_bias=False)(input_1)\n",
    "l_input_2 = keras.layers.Dense(1,activation='linear',use_bias=False,kernel_initializer=initializer,trainable=False)(input_2)\n",
    "division = keras.layers.Lambda(lambda inputs:  tf.where(inputs[0] != 0, inputs[1]/inputs[0], inputs[1]))([l_input_1, l_input_2])#calculates empirical rt\n",
    "#max_limit = keras.layers.Lambda(lambda inputs:  tf.where(inputs[0] > 4,4*inputs[1]/inputs[0], inputs[1]))([division,division]) #limit output to max of 4 for rt\n",
    "mean = keras.layers.Lambda(lambda x: keras.backend.dot(mat,x))(division) #calculates mean r_seven in seven days\n",
    "cases = keras.layers.Multiply()([input_3,mean]) #cases for valid prediction,net output, padded with zeros at the end\n",
    "model = keras.Model(inputs=[input_1,input_2,input_3], outputs=cases)\n",
    "optimizer = keras.optimizers.SGD(1e-7)\n",
    "model.compile(optimizer, loss=keras.losses.MSE, metrics=['mse'])\n",
    "model.fit([m1,m2,a], b, epochs=100, batch_size=n_dias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.0867482],\n",
      "       [1.1427605],\n",
      "       [1.113244 ]], dtype=float32)] [array([[1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "first_layer_weights = model.layers[2].get_weights()\n",
    "second_layer_weights = model.layers[3].get_weights()\n",
    "print(first_layer_weights,second_layer_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
